{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we‚Äôll present a step-by-step guide on fine-tuning an encoder-only transformer model for music classification. We‚Äôll use a lightweight model for this demonstration and fairly small dataset, meaning the code is runnable end-to-end on any consumer grade GPU, including the T4 16GB GPU provided in the Google Colab free tier. The section includes various tips that you can try should you have a smaller GPU and encounter memory issues along the way.\n",
        "\n"
      ],
      "metadata": {
        "id": "rx3EeZC5VBcJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Dataset\n",
        "\n",
        "To train our model, we‚Äôll use the GTZAN dataset, which is a popular dataset of 1,000 songs for music genre classification. Each song is a 30-second clip from one of 10 genres of music, spanning disco to metal. We can get the audio files and their corresponding labels from the Hugging Face Hub with the load_dataset() function from ü§ó Datasets:\n",
        "\n"
      ],
      "metadata": {
        "id": "T__mOEpOVXy3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQ4a_7ZIU0ma",
        "outputId": "84ff94d3-f313-4808-c5a4-d42d96ea1e01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m474.3/474.3 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q datasets[audio]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "gtzan = load_dataset(\"marsyas/gtzan\", \"all\")\n",
        "gtzan"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "IFsfJ59_VeP2",
        "outputId": "8f17dafa-3933-44bc-81f1-523ed86a02c8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "HfHubHTTPError",
          "evalue": "500 Server Error: Internal Server Error for url: https://huggingface.co/api/datasets/marsyas/gtzan (Request ID: Root=1-66eeb0cc-2eb9159069eae9295cd93cc7;337b1461-8758-4663-8440-e6ea5e64e829)\n\nInternal Error - We're working hard to fix this as soon as possible!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-ab0138d808c7>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgtzan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"marsyas/gtzan\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"all\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mgtzan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2073\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2074\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   2075\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2076\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   1793\u001b[0m         \u001b[0mdownload_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdownload_config\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mDownloadConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m         \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1795\u001b[0;31m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[1;32m   1796\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1797\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1669\u001b[0m                             \u001b[0;34mf\"Couldn't find '{path}' on the Hugging Face Hub either: {type(e1).__name__}: {e1}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1670\u001b[0m                         ) from None\n\u001b[0;32m-> 1671\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1672\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1673\u001b[0m         raise FileNotFoundError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1578\u001b[0m             \u001b[0mhf_api\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHfApi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHF_ENDPOINT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1579\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1580\u001b[0;31m                 dataset_info = hf_api.dataset_info(\n\u001b[0m\u001b[1;32m   1581\u001b[0m                     \u001b[0mrepo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1582\u001b[0m                     \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mdataset_info\u001b[0;34m(self, repo_id, revision, timeout, files_metadata, expand, token)\u001b[0m\n\u001b[1;32m   2444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2445\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2446\u001b[0;31m         \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2447\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDatasetInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;31m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;31m# as well (request id and/or server error message)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHfHubHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHfHubHTTPError\u001b[0m: 500 Server Error: Internal Server Error for url: https://huggingface.co/api/datasets/marsyas/gtzan (Request ID: Root=1-66eeb0cc-2eb9159069eae9295cd93cc7;337b1461-8758-4663-8440-e6ea5e64e829)\n\nInternal Error - We're working hard to fix this as soon as possible!"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GTZAN doesn‚Äôt provide a predefined validation set, so we‚Äôll have to create one ourselves. The dataset is balanced across genres, so we can use the train_test_split() method to quickly create a 90/10 split as follows:\n",
        "\n"
      ],
      "metadata": {
        "id": "AxHGKW51VpRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gtzan = gtzan['train'].train_test_split(seed=42, shuffle=True, test_size=0.1)\n",
        "gtzan"
      ],
      "metadata": {
        "id": "I3vvi43IVgXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great, now that we‚Äôve got our training and validation sets, let‚Äôs take a look at one of the audio files:\n",
        "\n"
      ],
      "metadata": {
        "id": "mcHOkcF2V6yu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gtzan['train'][0]"
      ],
      "metadata": {
        "id": "wYhqIg5lVxkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we saw in Unit 1, the audio files are represented as 1-dimensional NumPy arrays, where the value of the array represents the amplitude at that timestep. For these songs, the sampling rate is 22,050 Hz, meaning there are 22,050 amplitude values sampled per second. We‚Äôll have to keep this in mind when using a pretrained model with a different sampling rate, converting the sampling rates ourselves to ensure they match. We can also see the genre is represented as an integer, or class label, which is the format the model will make it‚Äôs predictions in. Let‚Äôs use the int2str() method of the genre feature to map these integers to human-readable names:\n",
        "\n"
      ],
      "metadata": {
        "id": "UKzsRlFNZm20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id2label_fn = gtzan['train'].features['genre'].int2str\n",
        "id2label_fn(gtzan['train'][0]['genre'])"
      ],
      "metadata": {
        "id": "hay5BwtCV9JZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This label looks correct, since it matches the filename of the audio file. Let‚Äôs now listen to a few more examples by using Gradio to create a simple interface with the Blocks API:\n",
        "\n"
      ],
      "metadata": {
        "id": "IrooZ8zLZ3MT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio"
      ],
      "metadata": {
        "id": "rWR7HIqjbbIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def generate_audio():\n",
        "  example = gtzan['train'].shuffle()[0]\n",
        "  audio = example['audio']\n",
        "\n",
        "  return (\n",
        "      audio['sampling_rate'],\n",
        "      audio['array']\n",
        "  ), id2label_fn(example['genre'])\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "  with gr.Column():\n",
        "    for _ in range(4):\n",
        "      audio, label = generate_audio()\n",
        "      output = gr.Audio(audio, label=label)\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "id": "5wGs1b_QZyjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From these samples we can certainly hear the difference between genres, but can a transformer do this too? Let‚Äôs train a model to find out! First, we‚Äôll need to find a suitable pretrained model for this task. Let‚Äôs see how we can do that.\n",
        "\n"
      ],
      "metadata": {
        "id": "kepIKw_rcUBN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Picking a pretrained model for audio classification\n",
        "To get started, let‚Äôs pick a suitable pretrained model for audio classification. In this domain, pretraining is typically carried out on large amounts of unlabeled audio data, using datasets like LibriSpeech and Voxpopuli. The best way to find these models on the Hugging Face Hub is to use the ‚ÄúAudio Classification‚Äù filter, as described in the previous section. Although models like Wav2Vec2 and HuBERT are very popular, we‚Äôll use a model called DistilHuBERT. This is a much smaller (or distilled) version of the HuBERT model, which trains around 73% faster, yet preserves most of the performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "Q3d-7ylzc0in"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From audio to machine learning features\n",
        "#### Preprocessing the data\n",
        "Similar to tokenization in NLP, audio and speech models require the input to be encoded in a format that the model can process. In ü§ó Transformers, the conversion from audio to the input format is handled by the feature extractor of the model. Similar to tokenizers, ü§ó Transformers provides a convenient AutoFeatureExtractor class that can automatically select the correct feature extractor for a given model. To see how we can process our audio files, let‚Äôs begin by instantiating the feature extractor for DistilHuBERT from the pre-trained checkpoint:"
      ],
      "metadata": {
        "id": "5dFjR0Kndvcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoFeatureExtractor\n",
        "\n",
        "model_id = \"ntu-spml/distilhubert\"\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
        "    model_id, do_normalize=True,\n",
        "    return_attention_mask=True\n",
        ")"
      ],
      "metadata": {
        "id": "juVnfCX4bZ31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the sampling rate of the model and the dataset are different, we‚Äôll have to resample the audio file to 16,000 Hz before passing it to the feature extractor. We can do this by first obtaining the model‚Äôs sample rate from the feature extractor:\n",
        "\n"
      ],
      "metadata": {
        "id": "Xs5cK5YLeWHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampling_rate = feature_extractor.sampling_rate\n",
        "sampling_rate"
      ],
      "metadata": {
        "id": "IfxWjSAFeKY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we resample the dataset using the cast_column() method and Audio feature from ü§ó Datasets:\n",
        "\n"
      ],
      "metadata": {
        "id": "vwgZiNcEedGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Audio\n",
        "\n",
        "gtzan = gtzan.cast_column('audio', Audio(sampling_rate=sampling_rate))"
      ],
      "metadata": {
        "id": "fFEoECTyebc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now check the first sample of the train-split of our dataset to verify that it is indeed at 16,000 Hz. ü§ó Datasets will resample the audio file on-the-fly when we load each audio sample:\n",
        "\n"
      ],
      "metadata": {
        "id": "Y0vgjBlZepQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gtzan[\"train\"][0]"
      ],
      "metadata": {
        "id": "lkc9hcqAenpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! We can see that the sampling rate has been downsampled to 16kHz. The array values are also different, as we‚Äôve now only got approximately one amplitude value for every 1.5 that we had before.\n",
        "\n"
      ],
      "metadata": {
        "id": "nOKvyWDDetzg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A defining feature of Wav2Vec2 and HuBERT like models is that they accept a float array corresponding to the raw waveform of the speech signal as an input. This is in contrast to other models, like Whisper, where we pre-process the raw audio waveform to spectrogram format.\n",
        "\n",
        "We mentioned that the audio data is represented as a 1-dimensional array, so it‚Äôs already in the right format to be read by the model (a set of continuous inputs at discrete time steps). So, what exactly does the feature extractor do?\n",
        "\n",
        "Well, the audio data is in the right format, but we‚Äôve imposed no restrictions on the values it can take. For our model to work optimally, we want to keep all the inputs within the same dynamic range. This is going to make sure we get a similar range of activations and gradients for our samples, helping with stability and convergence during training.\n",
        "\n",
        "To do this, we normalise our audio data, by rescaling each sample to zero mean and unit variance, a process called feature scaling. It‚Äôs exactly this feature normalisation that our feature extractor performs!\n"
      ],
      "metadata": {
        "id": "6xDX2OvqiImT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "sample = gtzan['train'][0]['audio']\n",
        "\n",
        "print(f\"Mean: {np.mean(sample['array']):.3}, Variance: {np.var(sample['array']):.3}\")"
      ],
      "metadata": {
        "id": "4KHIEXzpeqRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the mean is close to zero already, but the variance is closer to 0.05. If the variance for the sample was larger, it could cause our model problems, since the dynamic range of the audio data would be very small and thus difficult to separate. Let‚Äôs apply the feature extractor and see what the outputs look like:\n",
        "\n"
      ],
      "metadata": {
        "id": "ZTTKgxQaR7XZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = feature_extractor(sample['array'], sampling_rate=sample['sampling_rate'])\n",
        "\n",
        "print(f\"inputs keys: {list(inputs.keys())}\")\n",
        "print(\n",
        "    f\"Mean: {np.mean(inputs['input_values']):.3}, Variance: {np.var(inputs['input_values']):.3}\"\n",
        ")"
      ],
      "metadata": {
        "id": "xRgIXGzHiPmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the mean value is now very much closer to zero, and the variance bang-on one! This is exactly the form we want our audio samples in prior to feeding them to the HuBERT model.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZKSeMHf_SB1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great, so now we know how to process our resampled audio files, the last thing to do is define a function that we can apply to all the examples in the dataset. Since we expect the audio clips to be 30 seconds in length, we‚Äôll also truncate any longer clips by using the max_length and truncation arguments of the feature extractor as follows:\n",
        "\n"
      ],
      "metadata": {
        "id": "GDFALMgnSE_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_duration = 30.0\n",
        "\n",
        "def preprocess_function(examples):\n",
        "  audio_arrays = [x['array'] for x in examples['audio']]\n",
        "  inputs = feature_extractor(\n",
        "      audio_arrays,\n",
        "      sampling_rate=feature_extractor.sampling_rate,\n",
        "      max_length=int(feature_extractor.sampling_rate * max_duration),\n",
        "      truncation=True,\n",
        "      return_attention_mask=True\n",
        "  )\n",
        "\n",
        "  return inputs"
      ],
      "metadata": {
        "id": "1QHcxWoziebf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this function defined, we can now apply it to the dataset using the map() method. The .map() method supports working with batches of examples, which we‚Äôll enable by setting batched=True. The default batch size is 1000, but we‚Äôll reduce it to 100 to ensure the peak RAM stays within a sensible range for Google Colab‚Äôs free tier:\n"
      ],
      "metadata": {
        "id": "EBbi8jNzTV1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gtzan_encoded = gtzan.map(\n",
        "    preprocess_function,\n",
        "    remove_columns=['audio','file'],\n",
        "    batched=True,\n",
        "    batch_size=100,\n",
        "    num_proc=1\n",
        ")\n",
        "\n",
        "gtzan_encoded"
      ],
      "metadata": {
        "id": "viS-yu6NSoxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To simplify the training, we‚Äôve removed the audio and file columns from the dataset. The input_values column contains the encoded audio files, the attention_mask a binary mask of 0/1 values that indicate where we have padded the audio input, and the genre column contains the corresponding labels (or targets). To enable the Trainer to process the class labels, we need to rename the genre column to label:"
      ],
      "metadata": {
        "id": "QMTC2BlvUXvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gtzan_encoded = gtzan_encoded.rename_column(\"genre\", \"label\")"
      ],
      "metadata": {
        "id": "xekbCKStTp09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we need to obtain the label mappings from the dataset. This mapping will take us from integer ids (e.g. 7) to human-readable class labels (e.g. \"pop\") and back again. In doing so, we can convert our model‚Äôs integer id prediction into human-readable format, enabling us to use the model in any downstream application. We can do this by using the int2str() method as follows:\n",
        "\n"
      ],
      "metadata": {
        "id": "GAXZPPtIUxFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = {\n",
        "    str(i): id2label_fn(i)\n",
        "    for i in range(len(gtzan_encoded[\"train\"].features[\"label\"].names))\n",
        "}\n",
        "label2id = {v: k for k, v in id2label.items()}\n",
        "\n",
        "id2label[\"7\"]"
      ],
      "metadata": {
        "id": "rOF62ae6Urqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK, we‚Äôve now got a dataset that‚Äôs ready for training! Let‚Äôs take a look at how we can train a model on this dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "fIqT0d-2WXiu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning the model\n"
      ],
      "metadata": {
        "id": "knUirdU_WY_b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To fine-tune the model, we‚Äôll use the Trainer class from ü§ó Transformers. As we‚Äôve seen in other chapters, the Trainer is a high-level API that is designed to handle the most common training scenarios. In this case, we‚Äôll use the Trainer to fine-tune the model on GTZAN. To do this, we‚Äôll first need to load a model for this task. We can do this by using the AutoModelForAudioClassification class, which will automatically add the appropriate classification head to our pretrained DistilHuBERT model. Let‚Äôs go ahead and instantiate the model:"
      ],
      "metadata": {
        "id": "SkqgjQgGWih3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForAudioClassification\n",
        "\n",
        "num_labels = len(id2label)\n",
        "\n",
        "model = AutoModelForAudioClassification.from_pretrained(\n",
        "    model_id,\n",
        "    num_labels=num_labels,\n",
        "    label2id=label2id,\n",
        "    id2label=id2label,\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "HXToGfetWTx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "RreDvursW2mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "\n",
        "model_name = model_id.split(\"/\")[-1]\n",
        "batch_size = 8\n",
        "gradient_accumulation_steps = 1\n",
        "num_train_epochs = 10\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    f\"{model_name}-finetuned-gtzan\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    warmup_ratio=0.1,\n",
        "    logging_steps=5,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    fp16=True,\n",
        "    push_to_hub=True,\n",
        ")"
      ],
      "metadata": {
        "id": "qbkjJSFUW7Us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q evaluate"
      ],
      "metadata": {
        "id": "G0RLlQssYGB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
        "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)"
      ],
      "metadata": {
        "id": "ikk0ZUVyXvNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=gtzan_encoded[\"train\"],\n",
        "    eval_dataset=gtzan_encoded[\"test\"],\n",
        "    tokenizer=feature_extractor,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "5Ht6z6fEYCah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kwargs = {\n",
        "    \"dataset_tags\": \"marsyas/gtzan\",\n",
        "    \"dataset\": \"GTZAN\",\n",
        "    \"model_name\": f\"{model_name}-finetuned-gtzan\",\n",
        "    \"finetuned_from\": model_id,\n",
        "    \"tasks\": \"audio-classification\",\n",
        "}"
      ],
      "metadata": {
        "id": "mo4kltRzYckW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub(**kwargs)"
      ],
      "metadata": {
        "id": "HKTaZ0RUYfBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6pjExGDJYjcI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}